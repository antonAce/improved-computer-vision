{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hd39vLQ4ily"
      },
      "source": [
        "# Introduction and Overview of existing gradient algorithms\n",
        "\n",
        "In this assignment, we explore the evolution and significance of gradient descent algorithms, focusing on their applications in handling complex data-driven problems prevalent in fields such as machine learning and natural language processing. We will delve into the foundations of both classical and adaptive stochastic gradient techniques and investigating their convergence properties.\n",
        "\n",
        "### Historical Context\n",
        "\n",
        "Gradient descent algorithms have evolved significantly, starting from the stochastic approximation methods of Kiefer-Wolfowitz and Robbins-Monro in the 1950s, to the introduction of advanced techniques like Momentum Gradient Descent and Nesterov's accelerated method in the 1980s. The 2010s marked a shift towards adaptive methods, with algorithms like AdaGrad, RMSProp, and ADAM, each bringing unique approaches to learning rate adjustments and showcasing effectiveness in various applications, particularly in deep learning.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "1. Understand the fundamental concepts of gradient, gradient descent, and stochastic optimization;\n",
        "2. Explore the theoretical foundations and practical applications of various stochastic gradient descent algorithms;\n",
        "3. Compare the performance of different gradient descent algorithms on a test convex and smooth objective function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_quN7U1T9aH"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "Before delving into the implementation and comparison of gradient descent algorithms, it is essential to set up the necessary computational environment. We will be utilizing the PyTorch library to perform all calculations, as it offers a flexible and efficient platform for scientific computing, particularly in machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3jT6dGjUdf9"
      },
      "outputs": [],
      "source": [
        "# Import the PyTorch library\n",
        "import torch\n",
        "\n",
        "# Import typing annotations for assignment hints\n",
        "from typing import Callable, Tuple\n",
        "\n",
        "# Check the version of PyTorch\n",
        "print(\"PyTorch Version:\", torch.__version__)\n",
        "\n",
        "# Perform a basic operation to test PyTorch\n",
        "a = torch.tensor([1.0, 2.0])\n",
        "b = torch.tensor([3.0, 4.0])\n",
        "\n",
        "# Assert the result of the sum\n",
        "assert torch.equal(a + b, torch.tensor([4.0, 6.0])), \"The sum of a and b is incorrect!\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Algorithm convergence visualisation\n",
        "\n",
        "Iterations of the each gradient descent algorithm can be plotted as the path on 3D surface, in case model has only 2 independent parameters $\\{ \\theta_{1}, \\theta_{2} \\}$ so they can represent $X$ and $Y$ axises, and remaining $Z$ axis represents value of estimation equation to optimize $J(\\theta_{1}, \\theta_{2})$. All visualisation tools are used from [matplotlib](https://github.com/matplotlib/matplotlib) package."
      ],
      "metadata": {
        "id": "OnGZxtxoK-W_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as grid\n",
        "import matplotlib.cm as colormaps\n",
        "from matplotlib import rcParams, cycler\n",
        "\n",
        "from mpl_toolkits.mplot3d.axes3d import Axes3D"
      ],
      "metadata": {
        "id": "BoLA05oFK_hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To plot the projection of the 3D surface of $J(\\theta_{i})$ will be used a custom procedure."
      ],
      "metadata": {
        "id": "qNnPV14gLBhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_grid(F: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
        "              X: torch.Tensor, Y: torch.Tensor,\n",
        "              elev: int = 30, azim: int = 50, ax=None) -> Axes3D:\n",
        "  \"\"\"\n",
        "  Plots 3D surface grid for 2 independent parameters and estimation equation.\n",
        "\n",
        "  Args:\n",
        "      F (Callable[[torch.Tensor, torch.Tensor], torch.Tensor]): Estimation equation.\n",
        "      X (torch.Tensor): First independent parameter.\n",
        "      Y (torch.Tensor): Second independent parameter.\n",
        "      elev (int, optional): Vertical rotation angle. Defaults to 30.\n",
        "      azim (int, optional): Horizontal rotation angle. Defaults to 50.\n",
        "      ax (Axes3D, optional): Predefined plotting axis. If None, a new one will be created. Defaults to None.\n",
        "\n",
        "  Returns:\n",
        "      Axes3D: Generated plotting axis for potential reusability.\n",
        "  \"\"\"\n",
        "\n",
        "  # Generating grid\n",
        "  x, y = torch.meshgrid(X, Y, indexing='xy')\n",
        "\n",
        "  # If grid plotting axis is not defined above, the new one will be created\n",
        "  if ax is None:\n",
        "      fig = plt.figure()\n",
        "      ax = fig.add_subplot(111, projection='3d')\n",
        "      ax.view_init(elev=elev, azim=azim)\n",
        "\n",
        "  # Plotting grid\n",
        "  surf = ax.plot_surface(x.numpy(), y.numpy(), F(x, y).numpy(),\n",
        "                          cmap=colormaps.coolwarm,\n",
        "                          antialiased=True)\n",
        "  fig.colorbar(surf)\n",
        "\n",
        "  # For axis reusability purposes\n",
        "  return ax"
      ],
      "metadata": {
        "id": "5X0awryRLCtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zW8YIIsU4Fs"
      },
      "source": [
        "## Stochastic Optimization Problem\n",
        "\n",
        "Stochastic optimization problems form the bedrock for addressing uncertainties and randomness inherent in various domains like finance, machine learning, and operations research. Contrasting with deterministic optimization, where the objective function and constraints are well-defined, stochastic optimization introduces challenges by incorporating components that exhibit randomness. In this section, we delve into the mathematical formulation of a stochastic optimization problem and explore how stochastic gradient descent algorithms tackle the challenges presented by this formulation.\n",
        "\n",
        "### Mathematical Formulation\n",
        "\n",
        "Given an objective function $ f: X \\to \\mathbb{R} ^ n $ with domain $ X \\subset \\mathbb{R} ^ n $, and a convex and differentiable function $ F: X \\times \\Xi \\to \\mathbb{R} ^ 1 $ that depends on the determined variable $ x \\in X $ and a stochastic variable $ \\xi \\in \\Xi $, defined on a space $ (\\Xi, \\Sigma, P) $, a stochastic optimization problem can be represented as:\n",
        "\n",
        "$$\n",
        "\\min_{x \\in X} \\left[f(x) = \\mathbb{E} F(x, \\xi) = \\int_{\\xi \\in \\Xi} F(x, \\xi) P(d \\xi), X \\subset \\mathbb{R} ^ n\\right]\n",
        "$$\n",
        "\n",
        "Here, $ \\mathbb{E} $ denotes the mathematical expectation. The intrinsic challenge of this problem lies in the difficulty of explicitly calculating the value of an integral (mathematical expectation) and the gradient of this integral. Stochastic gradient descent algorithms, leveraging gradients $ \\nabla_{x} F(x, \\xi) $ of a stochastic function $ F(\\cdot, \\xi) $ or their finite-difference counterparts, offer a solution to this challenge.\n",
        "\n",
        "### Practical Implications\n",
        "\n",
        "These optimization problems are pivotal in scenarios where decision-making is dependent on incomplete or uncertain information. Employing techniques such as random sampling, Monte Carlo simulations, and stochastic gradients, stochastic optimization methods effectively and efficiently traverse the optimization landscape, aiming for convergence to the optimal solution.\n",
        "\n",
        "### Stochastic logistic regression\n",
        "\n",
        "Logistic regression is chosen as an example of an optimization problem for comparing the convergence rate of the gradient descent algorithms. This mathematical model can be described as a binary classifier, which outputs a probability value of a certain set of features $ x_{i} $ to belong to a certain class. Using the definition of optimization problem from above, the classifier can be represented in the form of two components: an adder that combines all the characteristics into a single one: $ z_{j} = \\sum_{i=1}^{n} \\theta_{i} x_{i, j} + \\varepsilon $ (or $ z_{j} = \\theta^{T} \\cdot x + \\varepsilon $), and a converter that calculates the probability of the characteristics belonging to a certain class based on the output value of an adder: $g(a) = \\frac{1}{1 + e^{-a}}$. Here $ \\varepsilon \\simeq N(0, 1) $ is a stochastic parameter in the model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def adder_fn(theta: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
        "    return theta.t() @ x\n",
        "\n",
        "def converter_fn(a: torch.Tensor) -> torch.Tensor:\n",
        "    return 1.0 / (1.0 + torch.exp(-a))\n",
        "\n",
        "# x = (0, 0), theta = (1, 1) => g = 0.5\n",
        "assert torch.allclose(\n",
        "    converter_fn(adder_fn(torch.tensor([[1.0], [1.0]]),\n",
        "                          torch.tensor([[0.0], [0.0]]))),\n",
        "    torch.tensor([[0.5]]))\n",
        "\n",
        "# x = (1, 0), theta = (1, 1) => g = 0.7310586\n",
        "assert torch.allclose(\n",
        "    converter_fn(adder_fn(torch.tensor([[1.0], [1.0]]),\n",
        "                          torch.tensor([[1.0], [0.0]]))),\n",
        "    torch.tensor([[0.7310586]]))\n",
        "\n",
        "# x = (0, -1), theta = (1, 1) => g = 0.26894143\n",
        "assert torch.allclose(\n",
        "    converter_fn(adder_fn(torch.tensor([[1.0], [1.0]]),\n",
        "                          torch.tensor([[0.0], [-1.0]]))),\n",
        "    torch.tensor([[0.26894143]]))\n",
        "\n",
        "# x = (0.5, -1), theta = (2, -1.5) => g = 0.9241418\n",
        "assert torch.allclose(\n",
        "    converter_fn(adder_fn(torch.tensor([[2.0], [-1.5]]),\n",
        "                          torch.tensor([[0.5], [-1.0]]))),\n",
        "    torch.tensor([[0.9241418]]))"
      ],
      "metadata": {
        "id": "4vpM-7PuLSVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The solution to the problem is to find optimal $\\theta_{i}$ so for input features $x_{i}$ the loss $J(\\theta_{i})$ between output value of classifier $\\hat{y_{j}} = g(z_{\\theta}(x_{i, j}))$ and expected value $y_{j}$ will be minimal.\n",
        "\n",
        "The set of classes, mentioned above, can be represented as $\\{ 0, 1 \\}$, e. g. if model output is $1$ - features belong to the class, and if $0$ - features do not belong to the class. In this case, the rule of the fitting regression model can be interpreted as follows: *if expected output is $y=1$, the loss should approach $0$ when model output approaches $1$ and grow to infinity when model output approaches $0$. For the case $y=0$ these conditions apply vise versa.*\n",
        "\n",
        "Model fitting rule or loss function can expressed as limits:\n",
        "\n",
        "$\\begin{cases}\n",
        "\\lim_{g(z_{\\theta}(x_{i, j})) \\to 1} J_{j}(\\theta_{i}) = 0, & \\lim_{g(z_{\\theta}(x_{i, j})) \\to 0} J_{j}(\\theta_{i}) = \\infty, & y=1 \\\\\n",
        "\\lim_{g(z_{\\theta}(x_{i, j})) \\to 0} J_{j}(\\theta_{i}) = 0, & \\lim_{g(z_{\\theta}(x_{i, j})) \\to 1} J_{j}(\\theta_{i}) = \\infty, & y=0\n",
        "\\end{cases}$\n",
        "\n",
        "Limitation conditions fit the $\\log(x)$ function, so the loss function can be expressed as the following system:\n",
        "\n",
        "$ J_{j}(g, y) = \\begin{cases}\n",
        "-\\log(g), & y = 1  \\\\\n",
        "-\\log(1 - g), & y = 0\n",
        "\\end{cases}$\n",
        "\n",
        "The statement can be joined into a single formula: $ J_{j}(g, y) = - y \\log(g) - (1 - y) \\log(1 - g) $ - this formula is appliable for single pair of features and output. For multiple pairs loss value can be calculated as the mean of losses for single pairs: $ J(g, y) = \\frac{1}{m} \\sum_{j=1}^{m} J_{j}(g, y) = \\frac{1}{m} \\sum_{j=1}^{m} ( - y \\log(g) - (1 - y) \\log(1 - g) ) = -\\frac{1}{m} \\sum_{j=1}^{m} (y \\log(g) + (1 - y) \\log(1 - g) )$"
      ],
      "metadata": {
        "id": "epSRoj5WOcpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(g: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes the binary cross-entropy loss between predicted probabilities and target labels.\n",
        "\n",
        "    Args:\n",
        "    g (torch.Tensor): The predicted probabilities.\n",
        "    y (torch.Tensor): The target labels.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: The computed binary cross-entropy loss.\n",
        "    \"\"\"\n",
        "\n",
        "    return -y * torch.log(g) - (1 - y) * torch.log(1 - g)"
      ],
      "metadata": {
        "id": "w83uOxyVOghb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the result, the derived fitting rule (loss function) is a convex function with only one minimum for each output class $ \\{ 0, 1 \\} $. This can be demonstraited on the following plots:"
      ],
      "metadata": {
        "id": "raV8G89HO2DY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate an array of values\n",
        "g = torch.arange(0.001, 1.0, 0.001)\n",
        "\n",
        "# Compute JLeft and JRight\n",
        "loss_left = -torch.log(g)\n",
        "loss_right = -torch.log(1 - g)\n",
        "\n",
        "# Plotting\n",
        "plt.plot(g.numpy(), loss_left.numpy(), lw=5, label='Left limit condition `y = 1`.')\n",
        "plt.plot(g.numpy(), loss_right.numpy(), lw=5, label='Left limit condition `y = 0`.')\n",
        "plt.xlabel('Converter output value')\n",
        "plt.ylabel(r'An objective function $ J(\\theta) $ value')\n",
        "plt.grid(True)\n",
        "plt.legend(loc='upper center')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z_WuGH_8O5KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss function is formed from combined limit conditions, which is plotted as a diagonal cross-section of the grid below:"
      ],
      "metadata": {
        "id": "0hJ8hWUlRW7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor y, ranging from 0.001 to 1.0 with steps of 0.001\n",
        "y = torch.arange(0.001, 1.0, 0.001)\n",
        "\n",
        "# Arguments for output equals 0\n",
        "yleft = torch.zeros(y.shape[0])\n",
        "\n",
        "# Arguments for output equals 1\n",
        "yright = torch.ones(y.shape[0])\n",
        "\n",
        "# Loss function plot in 3D cross-section\n",
        "ax = plot_grid(loss_fn, g, y, elev=30, azim=70)\n",
        "\n",
        "# Loss function plot for output equals 0\n",
        "ax.plot(g.numpy(), yleft.numpy(), loss_fn(g, yleft).numpy(), lw=5, label=\"loss for y=0\")\n",
        "\n",
        "# Loss function plot for output equals 1\n",
        "ax.plot(g.numpy(), yright.numpy(), loss_fn(g, yright).numpy(), lw=5, label=\"loss for y=1\")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W4an2-kBRbcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss calculation test cases:"
      ],
      "metadata": {
        "id": "lquvEfC9WhlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert torch.allclose(\n",
        "    loss_fn(torch.tensor([0.0001]), torch.tensor([0.0])),\n",
        "    torch.tensor([0.00010002]))\n",
        "\n",
        "assert torch.allclose(\n",
        "    loss_fn(torch.tensor([0.9998]), torch.tensor([1.0])),\n",
        "    torch.tensor([0.00019999]))\n",
        "\n",
        "assert torch.allclose(\n",
        "    loss_fn(torch.tensor([0.8]), torch.tensor([0.0])),\n",
        "    torch.tensor([1.609438]))\n",
        "\n",
        "assert torch.allclose(\n",
        "    loss_fn(torch.tensor([0.2]), torch.tensor([1.0])),\n",
        "    torch.tensor([1.609438]))"
      ],
      "metadata": {
        "id": "iB3c-3YsWirj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training dataset\n",
        "\n",
        "Necessary requirements for a set of training examples are:\n",
        "- Expected outputs should belong only to binary classes, e. g. $\\hat{y_{j}} \\in \\{ 0, 1 \\}$;\n",
        "- Input features should be defined as rational values $ \\{ x_{ij} | x_{ij} \\in R \\} $;\n",
        "\n",
        "For simplicity, a set of a training examples will be generated as feed-forward of regression model $g(z_{\\theta}(\\hat{\\theta_{i}}, x_{i, j}))$"
      ],
      "metadata": {
        "id": "eis4RZkzfsDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# To generate same random numbers per each run.\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Mean, variance and count of training examples\n",
        "mu, sigma, n = 0.0, 1.0, 50\n",
        "\n",
        "# Random noise\n",
        "e = torch.normal(mean=mu, std=sigma, size=(n,))\n",
        "\n",
        "# Mean value of the parameters\n",
        "theta = torch.tensor([2.0, 1.0])\n",
        "\n",
        "# Randomly distributed input features\n",
        "x_ = 4.0 * torch.rand((2, n)) - 2.0\n",
        "\n",
        "# Output labels as a direct feed-forward result\n",
        "y_ = 1.0 / (1.0 + torch.exp(-(theta @ x_)))\n",
        "\n",
        "# Expected output labels, use it as an output in the algorithms implementation\n",
        "y = torch.round(y_)\n",
        "\n",
        "# Input features with random noise, use it as an input in the algorithms implementation\n",
        "x = x_ + e"
      ],
      "metadata": {
        "id": "N_2JAkxwfulB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient approximation\n",
        "\n",
        "While libraries like PyTorch offer automatic differentiation, this assignment encourages a hands-on approach. We will be utilizing the second-order accurate central differences method to estimate gradients, offering insight into the intricacies of gradient computation and its role in optimization algorithms.\n",
        "\n",
        "We can derive approximation formula from the Taylor's series polynomial while discarding unnecessary residual terms that have higher accuracy order than the first order:\n",
        "\n",
        "$$ f(x_0 + h) = f(x_0) + f'(x_0)(h) + o(h) $$\n",
        "\n",
        "By applying finite differences approximation we get left and right finite differences:\n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "  f'_{-}(x) = \\frac{f(x + h) - f(x)}{h} - \\frac{h f''(\\xi)}{2} \\\\\n",
        "  f'_{+}(x) = \\frac{f(x) - f(x - h)}{h} + \\frac{h f''(\\xi)}{2}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "The accuracy of the approximation depends on the number of nodes on the numerical partitioning grid, thus the smaller step difference, the higher precision. Using the Runge-Romberg-Richardson algorithm we can achieve an increase in the order of the precision of the partitioning grid up to $ O(h^2) $ without adding extra iterations to the approximation algorithm:\n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "  f'_{-}(x) = \\frac{-3 f(x) + 4 f(x + h) - f(x + 2h)}{2h} + \\frac{h^2 f'''(\\xi)}{3} \\\\\n",
        "  f'(x) = \\frac{f(x + h) - f(x - h)}{2h} + \\frac{h^2 f'''(\\xi)}{6} \\\\\n",
        "  f'_{+}(x) = \\frac{f(x - 2h) - 4 f(x - h) + 3 f(x)}{2h} + \\frac{h^2 f'''(\\xi)}{3}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "The approach of approximating the gradient value of the target function with a finite-difference schema lets us generalize optimization problems on any kind of analytical functions."
      ],
      "metadata": {
        "id": "FVGDCDPOLNfL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hT1IpXQkA9W"
      },
      "outputs": [],
      "source": [
        "def grad_left(F: Callable[[torch.Tensor], torch.Tensor], x: torch.Tensor, h: float = 0.001) -> torch.Tensor:\n",
        "  \"\"\"A finite-difference approximation for left-side gradient ∇F₋(x) with the precision order O(h^2).\n",
        "\n",
        "  Args:\n",
        "      F (Callable[[torch.Tensor], torch.Tensor]): an objective function F(x) with a single input argument x ∈ ℝⁿ.\n",
        "      x (torch.Tensor): an input vector x ∈ ℝⁿ, where the derivative is calculated.\n",
        "      h (float, optional): a step of the derivative partitioning grid with the range of 0<h<1. The lower value, the higher gradient precision. Defaults to 0.001.\n",
        "\n",
        "  Returns:\n",
        "      torch.Tensor: a gradient vector approximation ∇F₋(x).\n",
        "  \"\"\"\n",
        "\n",
        "  pass # TODO: Implement second-order accurate forward difference algorithm\n",
        "\n",
        "\n",
        "def grad_center(F: Callable[[torch.Tensor], torch.Tensor], x: torch.Tensor, h: float = 0.001) -> torch.Tensor:\n",
        "  \"\"\"A finite-difference approximation for central gradient ∇F(x) with the precision order O(h^2).\n",
        "\n",
        "  Args:\n",
        "      F (Callable[[torch.Tensor], torch.Tensor]): a target function F(x) with a single input argument x ∈ ℝⁿ.\n",
        "      x (torch.Tensor): an input vector x ∈ ℝⁿ, where the derivative is calculated.\n",
        "      h (float, optional): a step of the derivative partitioning grid with the range of 0 < h < 1. The lower value, the higher gradient precision. Defaults to 0.001.\n",
        "\n",
        "  Returns:\n",
        "      torch.Tensor: a gradient vector approximation ∇F(x).\n",
        "  \"\"\"\n",
        "\n",
        "  n = x.shape[0]\n",
        "  grad = torch.zeros_like(x)\n",
        "\n",
        "  for i in range(n):\n",
        "      vh = h * torch.eye(n)[i]\n",
        "      grad[i] = (F(x + vh) - F(x - vh)) / (2.0 * h)\n",
        "\n",
        "  return grad\n",
        "\n",
        "\n",
        "def grad_right(F: Callable[[torch.Tensor], torch.Tensor], x: torch.Tensor, h: float = 0.001) -> torch.Tensor:\n",
        "  \"\"\"A finite-difference approximation for right-side gradient ∇F+(x) with the precision order O(h^2).\n",
        "\n",
        "  Args:\n",
        "      F (Callable[[torch.Tensor], torch.Tensor]): a target function F(x) with a single input argument x ∈ ℝⁿ.\n",
        "      x (torch.Tensor): an input vector x ∈ ℝⁿ, where the derivative is calculated.\n",
        "      h (float, optional): a step of the derivative partitioning grid with the range of 0<h<1. The lower value, the higher gradient precision. Defaults to 0.001.\n",
        "\n",
        "  Returns:\n",
        "      torch.Tensor: a gradient vector approximation ∇F+(x).\n",
        "  \"\"\"\n",
        "\n",
        "  pass # TODO: Implement second-order accurate center difference algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb2OyaXMp80i"
      },
      "source": [
        "We will check the accuracy of the implemented method on the test case:\n",
        "\n",
        "$ F(x, y) = x^2 + xy + y^2 \\implies \\begin{cases} \\frac{\\partial F(x, y)}{\\partial x} = 2x + y, \\frac{\\partial F(2.0, -1.0)}{\\partial x} = 3.0 \\\\ \\frac{\\partial F(x, y)}{\\partial y} = x + 2y, \\frac{\\partial F(2.0, -1.0)}{\\partial y} = 0.0\n",
        " \\end{cases} $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8k0BDMmqp7Ub"
      },
      "outputs": [],
      "source": [
        "h = 0.001\n",
        "x = torch.tensor([2.0, -1.0])\n",
        "f = lambda x: x[0] ** 2 + x[0] * x[1] + x[1] ** 2\n",
        "\n",
        "assert torch.allclose(grad_left(f, x, h=h), torch.tensor([3.0, 0.0]), rtol=h)\n",
        "assert torch.allclose(grad_center(f, x, h=h), torch.tensor([3.0, 0.0]), rtol=h)\n",
        "assert torch.allclose(grad_right(f, x, h=h), torch.tensor([3.0, 0.0]), rtol=h)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient descent variations\n",
        "\n",
        "Gradient descent algorithms have a common feature: using calculated gradient value as a direction towards the minimum of the function. The gradient value, by definition, is a vector that points towards the direction of the greatest increase of the function in a specific point. So the value, opposite to the gradient value or $ - \\nabla_{\\theta} J $, points towards the minimum.\n",
        "\n",
        "In general, gradient descent algorithm can be expressed as an iterative algorithm, that on each step $ \\theta_{j}^{(i)} $ moves in direction of $ - \\nabla_{\\theta} J $ with a fixed size step $ \\lambda $ (also known as learning rate) untill it reaches the minimum: $\\theta_{j}^{(i+1)} = \\theta_{j}^{(i)} - \\lambda \\nabla_{\\theta} J$.\n",
        "\n",
        "The well-known analogy of the algorithm in real world is a ball, rolling down the hill. In this case the gradient value $ - \\nabla_{\\theta} J $ representes rolling direction and learning rate $ \\lambda $ - the speed.\n",
        "\n",
        "Depending on how much data is used to compute the gradient of objective function, general form of an algorithm can variate, which leads to the possibility for a trade-off between the accuracy of the parameter update and the time it takes to perform an update."
      ],
      "metadata": {
        "id": "7zEluMDteIL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_function(_theta: torch.Tensor) -> torch.Tensor:\n",
        "  return loss_fn(converter_fn(adder_fn(_theta, x)), y)"
      ],
      "metadata": {
        "id": "cZq-442ooQUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch gradient descent\n",
        "\n",
        "Batch gradient descent or vanilla gradient descent performs computation of the gradient for $ \\nabla_{\\theta} J $ with respect to the independent variables $ \\theta_{j} $ for the entire set of training examples: $\\theta^{(i+1)} = \\theta^{(i)} - \\lambda \\nabla_{\\theta} J(\\theta, x, y)$\n",
        "\n",
        "Although batch gradient descent is guaranteed to converge to the global\n",
        "minimum for convex noisy surfaces, it requires to calculate gradient value for the entire set training examples, which makes it unefficient to calculate in parallel and, as the result, makes it very slow, because it can not update model parameters \"on-the-fly\". Another issue with this algorithm is an eager usage of memory, which means loading every training example into memory.\n",
        "\n",
        "Vanilla GD performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update, so its usage is efficient on a relatively small datasets.\n",
        "\n",
        "According to the original paper, BGD is guaranteed to converge to the global minimum for convex error surfaces and to a local minimum for non-convex surfaces."
      ],
      "metadata": {
        "id": "LQEloA9aoPux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_loss(_theta: torch.Tensor) -> torch.Tensor:\n",
        "  return torch.sum(objective_function(_theta)) / torch.tensor([n])\n",
        "\n",
        "\n",
        "eps = 0.001 # A numerical accuracy parameter: $ | x_i - x_{i-1} | < \\varepsilon $\n",
        "h = 0.0001 # A step of the derivative partitioning grid with the range of $ 0<h<1 $. The lower value, the higher gradient precision\n",
        "epoch, max_epoch = 0, 1000\n",
        "alpha = torch.tensor([0.01]) # A gradient descent step with a valid range of $ 0 < \\lambda < 1 $\n",
        "\n",
        "loss_hist = []\n",
        "theta_bgd_hist = torch.reshape(theta.detach().clone(), (1, theta.shape[0]))\n",
        "theta_bgd = theta.detach().clone()\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "  loss_hist.append(batch_loss(theta_bgd).detach().numpy().item())\n",
        "  grad_val = grad_center(batch_loss, theta_bgd) # TODO: Check other estimators and compare iterations number\n",
        "  theta_bgd -= alpha * grad_val\n",
        "\n",
        "  theta_bgd_hist = torch.cat((theta_bgd_hist, torch.reshape(theta_bgd, (1, theta.shape[0]))), dim=0)\n",
        "\n",
        "  if torch.norm(theta_bgd_hist[-1] - theta_bgd_hist[-2]) < eps:\n",
        "    break\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "axs[0].plot(list(range(1, len(loss_hist) + 1)), loss_hist)\n",
        "axs[0].set_xlabel('Iteration index')\n",
        "axs[0].set_ylabel('Loss function value')\n",
        "axs[0].set_title(f'Batch gradient descent converged at \\n {theta_bgd.detach().numpy().tolist()} with {epoch} iterations.',\n",
        "                 fontdict={ 'fontsize': 'small' })\n",
        "axs[0].grid(True)\n",
        "\n",
        "axs[1].plot(theta_bgd_hist[:, 0], theta_bgd_hist[:, 1])\n",
        "axs[1].set_xlabel('Theta 0')\n",
        "axs[1].set_ylabel('Theta 1')\n",
        "axs[1].set_title('The convergence trajectory', fontdict={ 'fontsize': 'small' }) # TODO: add surface plot of the objective function\n",
        "axs[1].grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3AamGd6UeNVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stochastic gradient descent\n",
        "\n",
        "Unlike algorithm from above, stochastic gradient descent (SGD) calculates gradient and performs parameters update for a single training example at once $( x_{i1}, ..., x_{in}, y_{i} ) $, where $ x_{i1}, ..., x_{in} $ are input features of a training example and $ y_{i} $ - label a training example: $\\theta^{(i+1)} = \\theta^{(i)} - \\lambda \\nabla_{\\theta} J(\\theta_{1}^{(i)}, ..., \\theta_{n}^{(i)}, x_{1}^{(i)}, ..., x_{n}^{(i)}, y^{(i)})$\n",
        "\n",
        "SGD removes redundancy for recomputing gradients for similar examples before each parameter update and also provides a possibility for a parallel computation of the gradient value for each example, which leads to updating model parameters \"on-the-fly\" (which lead to the faster computation).\n",
        "\n",
        "Nevertheless, this approach has its own flaw: frequent updates with a high variance that cause the objective function to fluctuate heavily On the one hand, SGD's fluctuation enables it to jump to a new and potentially better (global) minimum in a process of descending to the other local minimum. But on the other hand, this ultimately complicates convergence to the exact minimum, as SGD will keep overshooting. In practice, this flaw can be solved using learning rate decay methods (e. g. rate decreasing over epoch/iteration), though it requires precise parameter tuning."
      ],
      "metadata": {
        "id": "OM0BE39z2Ogc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eps = 0.001 # A numerical accuracy parameter: $ | x_i - x_{i-1} | < \\varepsilon $\n",
        "h = 0.0001 # A step of the derivative partitioning grid with the range of $ 0<h<1 $. The lower value, the higher gradient precision\n",
        "epoch, max_epoch = 0, 1000\n",
        "alpha = torch.tensor([0.01]) # A gradient descent step with a valid range of $ 0 < \\lambda < 1 $\n",
        "\n",
        "loss_hist = []\n",
        "theta_sgd_hist = torch.reshape(theta.detach().clone(), (1, theta.shape[0]))\n",
        "theta_sgd = theta.detach().clone()\n",
        "\n",
        "# TODO: Implement SGD"
      ],
      "metadata": {
        "id": "AdHogPK42RXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mini-batch gradient descent\n",
        "\n",
        "Mini-batch gradient descent is the golden ratio between BGD and SGD, because it inherits the best features from both algorithms. It performs an update for an every subset of training examples of a fixed size $m$:\n",
        "\n",
        "$\\theta^{(i+1)} = \\theta^{(i)} - \\lambda \\nabla_{\\theta} J(\\theta_{1}^{(i)}, ..., \\theta_{n}^{(i)}, x_{1}^{(j, j + m)}, ..., x_{n}^{(j, j + m)}, y^{(j, j + m)})$\n",
        "\n",
        "In this approach, the variance of the parameter updates is significantly reduced, which make converge of the algorithm more stable\n",
        "\n",
        "Although, this algorithm is a combination of the best features of BGD and SGD, it's not flawless. Mini-batch GD requires precised tuning of the learning rate (if it's too small - converge speed will significantly decrease, if it's too large - the loss function will fluctuate around minimum and, as a result, it will hinder convergence) and the algorithm not effective with noisy and highly non-convex functions, so the chance of being trapped in local minimum is bigger, than in other algorithms.\n",
        "\n",
        "As was shown in the original paper, there are other major challenges, that may occur:\n",
        " - The learning rate requires scheduling in order to make the algorithm reach the optimal point. The scheduler adjust it during training (by reducing the learning rate according to a pre-defined value or when the change in objective between epochs falls below a threshold);\n",
        " - The algorithm is not effective on sparse data (when features have different frequencies), because the same learning rate applies to all parameter updates;\n",
        " - The more saddle point function $J(\\theta_{1}^{(i)}, ..., \\theta_{n}^{(i)})$ have, the less effective algorithm becomes;"
      ],
      "metadata": {
        "id": "KHXgpmtE2UdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eps = 0.001 # A numerical accuracy parameter: $ | x_i - x_{i-1} | < \\varepsilon $\n",
        "h = 0.0001 # A step of the derivative partitioning grid with the range of $ 0<h<1 $. The lower value, the higher gradient precision\n",
        "epoch, max_epoch = 0, 1000\n",
        "alpha = torch.tensor([0.01]) # A gradient descent step with a valid range of $ 0 < \\lambda < 1 $\n",
        "\n",
        "loss_hist = []\n",
        "theta_mgd_hist = torch.reshape(theta.detach().clone(), (1, theta.shape[0]))\n",
        "theta_mgd = theta.detach().clone()\n",
        "\n",
        "# TODO: Implement MGD"
      ],
      "metadata": {
        "id": "ApTw47iJ2dlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SGD's variating descent, BGD's smooth descent, and MGD's moderate descent can be visualized on the comparison of loss function values over iterations and epochs accordingly."
      ],
      "metadata": {
        "id": "JOgPmipS2lAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compare trajectories on a single plot"
      ],
      "metadata": {
        "id": "52Ek8mNo2mwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accelerated gradient descent methods\n",
        "\n",
        "As was highlighted before, the common problems of the vanilla gradient descent variations are **learning rate tuning** and loss function fluctuations (for SGD and MGD). The recent researches in GD algorithms shown, that these problems can be solved and the convergence rate can be increased by using adaptive gradient calculation, which means applying gradient values from previous iterations to adjust the current one.\n",
        "\n",
        "### Momentum\n",
        "\n",
        "Returning back to the analogy of gradient descent algorithm as a ball, rolling down the hill, important to mention that it's not rolling with a constant speed $ \\lambda $, but instead gaining speed with time by keeping the momentum of itself. The same approach was discovered in optimization theory: the convergence rate of an algorithm can be increased by applying a fraction $ \\gamma $ of gradient from the previous iteration:\n",
        "\n",
        "$ v_{t} = \\gamma v_{t-1} + \\lambda \\nabla_{\\theta} J(\\theta_{1}^{(i)}, ..., \\theta_{n}^{(i)}, x_{1}^{(i)}, ..., x_{n}^{(i)}, y^{(i)}) $\n",
        "\n",
        "$ \\theta^{(i+1)} = \\theta^{(i)} - v_{t} $\n",
        "\n",
        "The momentum term $ \\gamma $ is usually set to 0.9 or a similar value.\n",
        "\n",
        "As mentioned in previous sections, SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around the local minimum. Momentum helps accelerate SGD in the relevant direction and dampens oscillations."
      ],
      "metadata": {
        "id": "cOMPIjNH3Bca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eps = 0.001 # A numerical accuracy parameter: $ | x_i - x_{i-1} | < \\varepsilon $\n",
        "h = 0.0001 # A step of the derivative partitioning grid with the range of $ 0<h<1 $. The lower value, the higher gradient precision\n",
        "epoch, max_epoch = 0, 1000\n",
        "alpha = torch.tensor([0.01]) # A gradient descent step with a valid range of $ 0 < \\lambda < 1 $\n",
        "gamma = torch.tensor([0.8]) # A momentum term\n",
        "\n",
        "loss_hist = []\n",
        "theta_momentum_hist = torch.reshape(theta.detach().clone(), (1, theta.shape[0]))\n",
        "theta_momentum = theta.detach().clone()\n",
        "\n",
        "# TODO: Implement Momentum algorithm"
      ],
      "metadata": {
        "id": "ZAb3LPFW3Kam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nesterov accelerated gradient\n",
        "\n",
        "In this approach, the momentum is not only applied to the calculated gradient value on the iteration, but also to the current parameters, which gives an approximation of their next position, like a preliminary step of the algorithm:\n",
        "\n",
        "$ v_{i} = \\gamma v_{i-1} + \\lambda \\nabla_{\\theta} J(\\theta^{(i)} - \\gamma v_{i-1}) $\n",
        "\n",
        "$ \\theta^{(i+1)} = \\theta^{(i)} - v_{i} $\n",
        "\n",
        "Taking the analogy from above, if a ball is made of light material, it may slope up to the other side of a hill using its momentum. But if it is made of heavy material (like steel), it will slow down near the bottom of the hill. The NAG algorithm work by the same principle. While momentum first computes the current gradient and then takes a big jump in the direction of the updated accumulated gradient, NAG first makes a big jump in the direction of the previously accumulated gradient, measures the gradient, and then makes a correction. This anticipatory update prevents us from going too fast and results in increased responsiveness."
      ],
      "metadata": {
        "id": "sSV-3zF23fTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eps = 0.001 # A numerical accuracy parameter: $ | x_i - x_{i-1} | < \\varepsilon $\n",
        "h = 0.0001 # A step of the derivative partitioning grid with the range of $ 0<h<1 $. The lower value, the higher gradient precision\n",
        "epoch, max_epoch = 0, 1000\n",
        "alpha = torch.tensor([0.01]) # A gradient descent step with a valid range of $ 0 < \\lambda < 1 $\n",
        "gamma = torch.tensor([0.8]) # A momentum term\n",
        "\n",
        "loss_hist = []\n",
        "theta_nag_hist = torch.reshape(theta.detach().clone(), (1, theta.shape[0]))\n",
        "theta_nag = theta.detach().clone()\n",
        "\n",
        "# TODO: Implement NAG algorithm"
      ],
      "metadata": {
        "id": "kp05dGFP3iRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient methods with adaptive learning rate\n",
        "\n",
        "In the previous modification of **SGD**, the learning rate $ \\lambda $ was fixed in size, so the problem with tuning it is still relevant to avoid either slow convergence or complete diverge. The following method modifications tend to solve this major issue.\n",
        "\n",
        "### AdaGrad\n",
        "\n",
        "The AdaGrad algorithm adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters: given learning rate $ \\lambda $ value is divided on some term $ G_{i} $ that accumulates gradient values from previous iterations:\n",
        "\n",
        "$ \\theta^{(i+1)} = \\theta^{(i)} - \\frac{\\lambda}{\\sqrt{G_{i} + \\epsilon}} \\odot g_{i} $, where $ g_{i} = \\nabla_{\\theta} J(\\theta^{(i)})$, $ \\epsilon $ is a smoothing term (a small coefficient, usually equals $ 3 \\cdot 10^{-6} $, that prevents the denominator from turning to zero) and  $ G_{i} $ is a combination of the previously accumulated gradient terms and element-wise product of $ g_{i} $ on itself: $ G_{i} = G_{i-1} + g_{i} \\cdot g_{i} = G_{i-1} + (g_{i})^{2} $\n",
        "\n",
        "Although the mentioned problem with tuning learning rate $ \\lambda $ is solved (in practice, the value is set to default 0.01 and left like that), the new one appears: accumulation of the squared gradients in the denominator causes $ \\lambda $ to shrink with every iteration (since every term is positive) and eventually is approaching zero. It can be illustrated on the given optimization problem for a small set of training examples: the learning rate $ \\lambda $ should be big enough ($ \\lambda $ = 0.1 for this example) to reach an area near the global minimum."
      ],
      "metadata": {
        "id": "5SpWlQMg37pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eps = 0.001 # A numerical accuracy parameter: $ | x_i - x_{i-1} | < \\varepsilon $\n",
        "h = 0.0001 # A step of the derivative partitioning grid with the range of $ 0<h<1 $. The lower value, the higher gradient precision\n",
        "epoch, max_epoch = 0, 1000\n",
        "alpha = torch.tensor([0.1]) # A gradient descent step with a valid range of $ 0 < \\lambda < 1 $\n",
        "\n",
        "loss_hist = []\n",
        "theta_adagrad_hist = torch.reshape(theta.detach().clone(), (1, theta.shape[0]))\n",
        "theta_adagrad = theta.detach().clone()\n",
        "\n",
        "# TODO: Implement AdaGrad algorithm"
      ],
      "metadata": {
        "id": "69IUua-t3-N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RMSProp\n",
        "\n",
        "A serious drawback of the AdaGrad algorithm is and exponential decay of a learning rate $ \\lambda $, due to the accumulation of the gradient values and uncontrollable growth in the denominator. On the one hand, rate decay results in reducing fluctuations in the objective function, but on the other hand, the rate becomes negligibly small after some iterations, so the algorithm may never reach the global minimum.\n",
        "\n",
        "In the RMSProp (Root Mean Square Propagation) algorithm this issue is solved by using the mean value of gradients in the denominator from the previous iterations instead of constant accumulation:\n",
        "\n",
        "$ E(g^{2})_{i} = \\beta E(g^{2})_{i-1} + (1 - \\beta) g_{i}^{2} $\n",
        "\n",
        "$ \\theta^{(i+1)} = \\theta^{(i)} - \\frac{\\lambda}{\\sqrt{E(g^{2})_{i} + \\epsilon}} \\odot g_{i} $\n",
        "\n",
        "where $ g_{i} = \\nabla_{\\theta} J(\\theta^{(i)})$, $ \\epsilon $ is a smoothing term, and $ \\beta $ is a momentum term (according to the original research, the optimal value is 0.9"
      ],
      "metadata": {
        "id": "bDX0YaE84iyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eps = 0.001 # A numerical accuracy parameter: $ | x_i - x_{i-1} | < \\varepsilon $\n",
        "h = 0.0001 # A step of the derivative partitioning grid with the range of $ 0<h<1 $. The lower value, the higher gradient precision\n",
        "epoch, max_epoch = 0, 1000\n",
        "alpha = torch.tensor([0.01]) # A gradient descent step with a valid range of $ 0 < \\lambda < 1 $\n",
        "beta = torch.tensor([0.9]) # A momentum term\n",
        "\n",
        "loss_hist = []\n",
        "theta_rmsprop_hist = torch.reshape(theta.detach().clone(), (1, theta.shape[0]))\n",
        "theta_rmsprop = theta.detach().clone()\n",
        "\n",
        "# TODO: Implement RMSProp algorithm"
      ],
      "metadata": {
        "id": "M_PKemWF46s7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ADAM\n",
        "\n",
        "The ADAM (Adaptive Moment Estimation) algorithm adapts not only the learning rate $ \\lambda $ to the parameters (like AdaGrad and RMSProp) also keeps an exponentially decaying average of past gradients $ m_{i} $, similar to momentum:\n",
        "\n",
        "$ m_{i} = \\beta_{1} m_{i-1} + (1 - \\beta_{1}) g_{i} $\n",
        "\n",
        "$ v_{i} = \\beta_{2} v_{i-1} + (1 - \\beta_{2}) g_{i}^{2} $\n",
        "\n",
        "where $ g_{i} = \\nabla_{\\theta} J(\\theta^{(i)})$, $ m_{i} $ - mean value estimation (first moment) of the gradients respectively, $ v_{i} $ - the uncentered variance estimation (second moment) of the gradients respectively.\n",
        "\n",
        "The original research was shown, that moment estimations are biased towards zero on the first iterations, especially when terms $ \\beta_{1} $, $ \\beta_{2} $ are small. So in the descent algorithm, bias-corrected estimates are used:\n",
        "\n",
        "$ \\hat{m_{i}} = \\frac{m_{i}}{1 - \\beta_{1}} $\n",
        "\n",
        "$ \\hat{v_{i}} = \\frac{v_{i}}{1 - \\beta_{2}} $\n",
        "\n",
        "Now update rule for parameters can be modified using adaptive estimations:\n",
        "\n",
        "$ \\theta^{(i+1)} = \\theta^{(i)} - \\frac{\\lambda}{\\sqrt{\\hat{v_{i}} + \\epsilon}} \\hat{m_{i}} $\n",
        "\n",
        "From the empirical point of view, default estimation terms for the algorithm are equal: $ \\beta_{1} = 0.9 $, $ \\beta_{2} = 0.999 $.\n",
        "\n",
        "For the given optimization problem, the approach with adaptive estimations for learning rate $ \\lambda $ and for momentum will result in not only efficient convergence on the sparse data but also in adaptive decay of the momentum when the algorithm is approaching the minimum point (so the algorithm will not jump over the minimum, unlike pure momentum algorithm)."
      ],
      "metadata": {
        "id": "ogOUaB_I5O_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eps = 0.001 # A numerical accuracy parameter: $ | x_i - x_{i-1} | < \\varepsilon $\n",
        "h = 0.0001 # A step of the derivative partitioning grid with the range of $ 0<h<1 $. The lower value, the higher gradient precision\n",
        "epoch, max_epoch = 0, 1000\n",
        "alpha = torch.tensor([0.01]) # A gradient descent step with a valid range of $ 0 < \\lambda < 1 $\n",
        "beta1 = torch.tensor([0.9]) # A momentum term for the mean value\n",
        "beta2 = torch.tensor([0.999]) # A momentum term for the variance value\n",
        "\n",
        "loss_hist = []\n",
        "theta_adam_hist = torch.reshape(theta.detach().clone(), (1, theta.shape[0]))\n",
        "theta_adam = theta.detach().clone()\n",
        "\n",
        "# TODO: Implement ADAM algorithm"
      ],
      "metadata": {
        "id": "vOMZfgPx5RzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "`TODO: Compare trajectories of each method, solve regression using scikit-learn and compare the accuracy using RMSE. Display accuracy as a bar chart.`"
      ],
      "metadata": {
        "id": "ZuFLLwgY51C1"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}