{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hd39vLQ4ily"
      },
      "source": [
        "# Introduction and Overview of existing gradient algorithms\n",
        "\n",
        "In this assignment, we explore the evolution and significance of gradient descent algorithms, focusing on their applications in handling complex data-driven problems prevalent in fields such as machine learning and natural language processing. We will delve into the foundations of both classical and adaptive stochastic gradient techniques and investigating their convergence properties.\n",
        "\n",
        "### Historical Context\n",
        "\n",
        "Gradient descent algorithms have evolved significantly, starting from the stochastic approximation methods of Kiefer-Wolfowitz and Robbins-Monro in the 1950s, to the introduction of advanced techniques like Momentum Gradient Descent and Nesterov's accelerated method in the 1980s. The 2010s marked a shift towards adaptive methods, with algorithms like AdaGrad, RMSProp, and ADAM, each bringing unique approaches to learning rate adjustments and showcasing effectiveness in various applications, particularly in deep learning.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "1. Understand the fundamental concepts of gradient, gradient descent, and stochastic optimization;\n",
        "2. Explore the theoretical foundations and practical applications of various stochastic gradient descent algorithms;\n",
        "3. Compare the performance of different gradient descent algorithms on a test convex and smooth objective function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_quN7U1T9aH"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "Before delving into the implementation and comparison of gradient descent algorithms, it is essential to set up the necessary computational environment. We will be utilizing the PyTorch library to perform all calculations, as it offers a flexible and efficient platform for scientific computing, particularly in machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3jT6dGjUdf9"
      },
      "outputs": [],
      "source": [
        "# Import the PyTorch library\n",
        "import torch\n",
        "\n",
        "# Import typing annotations for assignment hints\n",
        "from typing import Callable\n",
        "\n",
        "# Check the version of PyTorch\n",
        "print(\"PyTorch Version:\", torch.__version__)\n",
        "\n",
        "# Perform a basic operation to test PyTorch\n",
        "a = torch.tensor([1.0, 2.0])\n",
        "b = torch.tensor([3.0, 4.0])\n",
        "\n",
        "# Assert the result of the sum\n",
        "assert torch.equal(a + b, torch.tensor([4.0, 6.0])), \"The sum of a and b is incorrect!\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Algorithm convergence visualisation\n",
        "\n",
        "Iterations of the each **gradient descent** algorithm can be plotted as the path on 3D surface, in case model has only 2 independent parameters $\\{ \\theta_{1}, \\theta_{2} \\}$ so they can represent $X$ and $Y$ axises, and remaining $Z$ axis represents value of estimation equation to optimize $J(\\theta_{1}, \\theta_{2})$. All visualisation tools are used from [matplotlib](https://github.com/matplotlib/matplotlib) package."
      ],
      "metadata": {
        "id": "OnGZxtxoK-W_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as grid\n",
        "import matplotlib.cm as colormaps\n",
        "from matplotlib import rcParams, cycler\n",
        "\n",
        "from mpl_toolkits.mplot3d.axes3d import Axes3D"
      ],
      "metadata": {
        "id": "BoLA05oFK_hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To plot the projection of the 3D surface of $J(\\theta_{i})$ will be used a custom procedure."
      ],
      "metadata": {
        "id": "qNnPV14gLBhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_grid(F: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
        "              X: torch.Tensor, Y: torch.Tensor,\n",
        "              elev: int = 30, azim: int = 50, ax=None) -> Axes3D:\n",
        "  \"\"\"\n",
        "  Plots 3D surface grid for 2 independent parameters and estimation equation.\n",
        "\n",
        "  Args:\n",
        "      F (Callable[[torch.Tensor, torch.Tensor], torch.Tensor]): Estimation equation.\n",
        "      X (torch.Tensor): First independent parameter.\n",
        "      Y (torch.Tensor): Second independent parameter.\n",
        "      elev (int, optional): Vertical rotation angle. Defaults to 30.\n",
        "      azim (int, optional): Horizontal rotation angle. Defaults to 50.\n",
        "      ax (Axes3D, optional): Predefined plotting axis. If None, a new one will be created. Defaults to None.\n",
        "\n",
        "  Returns:\n",
        "      Axes3D: Generated plotting axis for potential reusability.\n",
        "  \"\"\"\n",
        "\n",
        "  # Generating grid\n",
        "  x, y = torch.meshgrid(X, Y, indexing='xy')\n",
        "\n",
        "  # If grid plotting axis is not defined above, the new one will be created\n",
        "  if ax is None:\n",
        "      fig = plt.figure()\n",
        "      ax = fig.add_subplot(111, projection='3d')\n",
        "      ax.view_init(elev=elev, azim=azim)\n",
        "\n",
        "  # Plotting grid\n",
        "  surf = ax.plot_surface(x.numpy(), y.numpy(), F(x, y).numpy(),\n",
        "                          cmap=colormaps.coolwarm,\n",
        "                          antialiased=True)\n",
        "  fig.colorbar(surf)\n",
        "\n",
        "  # For axis reusability purposes\n",
        "  return ax"
      ],
      "metadata": {
        "id": "5X0awryRLCtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zW8YIIsU4Fs"
      },
      "source": [
        "## Stochastic Optimization Problem\n",
        "\n",
        "Stochastic optimization problems form the bedrock for addressing uncertainties and randomness inherent in various domains like finance, machine learning, and operations research. Contrasting with deterministic optimization, where the objective function and constraints are well-defined, stochastic optimization introduces challenges by incorporating components that exhibit randomness. In this section, we delve into the mathematical formulation of a stochastic optimization problem and explore how stochastic gradient descent algorithms tackle the challenges presented by this formulation.\n",
        "\n",
        "### Mathematical Formulation\n",
        "\n",
        "Given an objective function $ f: X \\to \\mathbb{R} ^ n $ with domain $ X \\subset \\mathbb{R} ^ n $, and a convex and differentiable function $ F: X \\times \\Xi \\to \\mathbb{R} ^ 1 $ that depends on the determined variable $ x \\in X $ and a stochastic variable $ \\xi \\in \\Xi $, defined on a space $ (\\Xi, \\Sigma, P) $, a stochastic optimization problem can be represented as:\n",
        "\n",
        "$$\n",
        "\\min_{x \\in X} \\left[f(x) = \\mathbb{E} F(x, \\xi) = \\int_{\\xi \\in \\Xi} F(x, \\xi) P(d \\xi), X \\subset \\mathbb{R} ^ n\\right]\n",
        "$$\n",
        "\n",
        "Here, $ \\mathbb{E} $ denotes the mathematical expectation. The intrinsic challenge of this problem lies in the difficulty of explicitly calculating the value of an integral (mathematical expectation) and the gradient of this integral. Stochastic gradient descent algorithms, leveraging gradients $ \\nabla_{x} F(x, \\xi) $ of a stochastic function $ F(\\cdot, \\xi) $ or their finite-difference counterparts, offer a solution to this challenge.\n",
        "\n",
        "### Practical Implications\n",
        "\n",
        "These optimization problems are pivotal in scenarios where decision-making is dependent on incomplete or uncertain information. Employing techniques such as random sampling, Monte Carlo simulations, and stochastic gradients, stochastic optimization methods effectively and efficiently traverse the optimization landscape, aiming for convergence to the optimal solution.\n",
        "\n",
        "### Stochastic logistic regression\n",
        "\n",
        "Logistic regression is chosen as an example of an optimization problem for comparing the convergence rate of the gradient descent algorithms. This mathematical model can be described as a binary classifier, which outputs a probability value of a certain set of features $ x_{i} $ to belong to a certain class. Using the definition of optimization problem from above, the classifier can be represented in the form of two components: an adder that combines all the characteristics into a single one: $ z_{j} = \\sum_{i=1}^{n} \\theta_{i} x_{i, j} + \\varepsilon $ (or $ z_{j} = \\theta^{T} \\cdot x + \\varepsilon $), and a converter that calculates the probability of the characteristics belonging to a certain class based on the output value of an adder: $g(a) = \\frac{1}{1 + e^{-a}}$. Here $ \\varepsilon \\simeq N(0, 1) $ is a stochastic parameter in the model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def adder_fn(theta: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
        "    return theta.t() @ x\n",
        "\n",
        "def converter_fn(a: torch.Tensor) -> torch.Tensor:\n",
        "    return 1.0 / (1.0 + torch.exp(-a))\n",
        "\n",
        "# x = (0, 0), theta = (1, 1) => g = 0.5\n",
        "assert torch.allclose(\n",
        "    converter_fn(adder_fn(torch.tensor([[1.0], [1.0]]),\n",
        "                          torch.tensor([[0.0], [0.0]]))),\n",
        "    torch.tensor([[0.5]]))\n",
        "\n",
        "# x = (1, 0), theta = (1, 1) => g = 0.7310586\n",
        "assert torch.allclose(\n",
        "    converter_fn(adder_fn(torch.tensor([[1.0], [1.0]]),\n",
        "                          torch.tensor([[1.0], [0.0]]))),\n",
        "    torch.tensor([[0.7310586]]))\n",
        "\n",
        "# x = (0, -1), theta = (1, 1) => g = 0.26894143\n",
        "assert torch.allclose(\n",
        "    converter_fn(adder_fn(torch.tensor([[1.0], [1.0]]),\n",
        "                          torch.tensor([[0.0], [-1.0]]))),\n",
        "    torch.tensor([[0.26894143]]))\n",
        "\n",
        "# x = (0.5, -1), theta = (2, -1.5) => g = 0.9241418\n",
        "assert torch.allclose(\n",
        "    converter_fn(adder_fn(torch.tensor([[2.0], [-1.5]]),\n",
        "                          torch.tensor([[0.5], [-1.0]]))),\n",
        "    torch.tensor([[0.9241418]]))"
      ],
      "metadata": {
        "id": "4vpM-7PuLSVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The solution to the problem is to find optimal $\\theta_{i}$ so for input features $x_{i}$ the **loss** $J(\\theta_{i})$ between output value of classifier $\\hat{y_{j}} = g(z_{\\theta}(x_{i, j}))$ and expected value $y_{j}$ will be minimal.\n",
        "\n",
        "The set of classes, mentioned above, can be represented as $\\{ 0, 1 \\}$, e. g. if model output is $1$ - features belong to the class, and if $0$ - features do not belong to the class. In this case, the rule of the fitting regression model can be interpreted as follows: *if expected output is $y=1$, the loss should approach $0$ when model output approaches $1$ and grow to infinity when model output approaches $0$. For the case $y=0$ these conditions apply vise versa.*\n",
        "\n",
        "Model fitting rule or **loss function** can expressed as limits:\n",
        "\n",
        "$\\begin{cases}\n",
        "\\lim_{g(z_{\\theta}(x_{i, j})) \\to 1} J_{j}(\\theta_{i}) = 0, & \\lim_{g(z_{\\theta}(x_{i, j})) \\to 0} J_{j}(\\theta_{i}) = \\infty, & y=1 \\\\\n",
        "\\lim_{g(z_{\\theta}(x_{i, j})) \\to 0} J_{j}(\\theta_{i}) = 0, & \\lim_{g(z_{\\theta}(x_{i, j})) \\to 1} J_{j}(\\theta_{i}) = \\infty, & y=0\n",
        "\\end{cases}$\n",
        "\n",
        "Limitation conditions fit the $\\log(x)$ function, so the **loss function** can be expressed as the following system:\n",
        "\n",
        "$ J_{j}(g, y) = \\begin{cases}\n",
        "-\\log(g), & y = 1  \\\\\n",
        "-\\log(1 - g), & y = 0\n",
        "\\end{cases}$\n",
        "\n",
        "The statement can be joined into a single formula: $ J_{j}(g, y) = - y \\log(g) - (1 - y) \\log(1 - g) $ - this formula is appliable for single pair of features and output. For multiple pairs loss value can be calculated as the mean of losses for single pairs: $ J(g, y) = \\frac{1}{m} \\sum_{j=1}^{m} J_{j}(g, y) = \\frac{1}{m} \\sum_{j=1}^{m} ( - y \\log(g) - (1 - y) \\log(1 - g) ) = -\\frac{1}{m} \\sum_{j=1}^{m} (y \\log(g) + (1 - y) \\log(1 - g) )$"
      ],
      "metadata": {
        "id": "epSRoj5WOcpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(g: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes the binary cross-entropy loss between predicted probabilities and target labels.\n",
        "\n",
        "    Args:\n",
        "    g (torch.Tensor): The predicted probabilities.\n",
        "    y (torch.Tensor): The target labels.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: The computed binary cross-entropy loss.\n",
        "    \"\"\"\n",
        "\n",
        "    return -y * torch.log(g) - (1 - y) * torch.log(1 - g)"
      ],
      "metadata": {
        "id": "w83uOxyVOghb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the result, the derived fitting rule (**loss function**) is a convex function with only one minimum for each output class $ \\{ 0, 1 \\} $. This can be demonstraited on the following plots:"
      ],
      "metadata": {
        "id": "raV8G89HO2DY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate an array of values\n",
        "g = torch.arange(0.001, 1.0, 0.001)\n",
        "\n",
        "# Compute JLeft and JRight\n",
        "loss_left = -torch.log(g)\n",
        "loss_right = -torch.log(1 - g)\n",
        "\n",
        "# Plotting\n",
        "plt.plot(g.numpy(), loss_left.numpy(), lw=5, label='Left limit condition `y = 1`.')\n",
        "plt.plot(g.numpy(), loss_right.numpy(), lw=5, label='Left limit condition `y = 0`.')\n",
        "plt.xlabel('Converter output value')\n",
        "plt.ylabel(r'An objective function $ J(\\theta) $ value')\n",
        "plt.grid(True)\n",
        "plt.legend(loc='upper center')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z_WuGH_8O5KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss function is formed from combined limit conditions, which is plotted as a diagonal cross-section of the grid below:"
      ],
      "metadata": {
        "id": "0hJ8hWUlRW7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor y, ranging from 0.001 to 1.0 with steps of 0.001\n",
        "y = torch.arange(0.001, 1.0, 0.001)\n",
        "\n",
        "# Arguments for output equals 0\n",
        "yleft = torch.zeros(y.shape[0])\n",
        "\n",
        "# Arguments for output equals 1\n",
        "yright = torch.ones(y.shape[0])\n",
        "\n",
        "# Loss function plot in 3D cross-section\n",
        "ax = plot_grid(loss_fn, g, y, elev=30, azim=70)\n",
        "\n",
        "# Loss function plot for output equals 0\n",
        "ax.plot(g.numpy(), yleft.numpy(), loss_fn(g, yleft).numpy(), lw=5, label=\"loss for y=0\")\n",
        "\n",
        "# Loss function plot for output equals 1\n",
        "ax.plot(g.numpy(), yright.numpy(), loss_fn(g, yright).numpy(), lw=5, label=\"loss for y=1\")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W4an2-kBRbcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss calculation test cases:"
      ],
      "metadata": {
        "id": "lquvEfC9WhlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert torch.allclose(\n",
        "    loss_fn(torch.tensor([0.0001]), torch.tensor([0.0])),\n",
        "    torch.tensor([0.00010002]))\n",
        "\n",
        "assert torch.allclose(\n",
        "    loss_fn(torch.tensor([0.9998]), torch.tensor([1.0])),\n",
        "    torch.tensor([0.00019999]))\n",
        "\n",
        "assert torch.allclose(\n",
        "    loss_fn(torch.tensor([0.8]), torch.tensor([0.0])),\n",
        "    torch.tensor([1.609438]))\n",
        "\n",
        "assert torch.allclose(\n",
        "    loss_fn(torch.tensor([0.2]), torch.tensor([1.0])),\n",
        "    torch.tensor([1.609438]))"
      ],
      "metadata": {
        "id": "iB3c-3YsWirj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient approximation\n",
        "\n",
        "While libraries like PyTorch offer automatic differentiation, this assignment encourages a hands-on approach. We will be utilizing the second-order accurate central differences method to estimate gradients, offering insight into the intricacies of gradient computation and its role in optimization algorithms.\n",
        "\n",
        "We can derive approximation formula from the Taylor's series polynomial while discarding unnecessary residual terms that have higher accuracy order than the first order:\n",
        "\n",
        "$$ f(x_0 + h) = f(x_0) + f'(x_0)(h) + o(h) $$\n",
        "\n",
        "By applying finite differences approximation we get left and right finite differences:\n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "  f'_{-}(x) = \\frac{f(x + h) - f(x)}{h} - \\frac{h f''(\\xi)}{2} \\\\\n",
        "  f'_{+}(x) = \\frac{f(x) - f(x - h)}{h} + \\frac{h f''(\\xi)}{2}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "The accuracy of the approximation depends on the number of nodes on the numerical partitioning grid, thus the smaller step difference, the higher precision. Using the Runge-Romberg-Richardson algorithm we can achieve an increase in the order of the precision of the partitioning grid up to $ O(h^2) $ without adding extra iterations to the approximation algorithm:\n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "  f'_{-}(x) = \\frac{-3 f(x) + 4 f(x + h) - f(x + 2h)}{2h} + \\frac{h^2 f'''(\\xi)}{3} \\\\\n",
        "  f'(x) = \\frac{f(x + h) - f(x - h)}{2h} + \\frac{h^2 f'''(\\xi)}{6} \\\\\n",
        "  f'_{+}(x) = \\frac{f(x - 2h) - 4 f(x - h) + 3 f(x)}{2h} + \\frac{h^2 f'''(\\xi)}{3}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "The approach of approximating the gradient value of the target function with a finite-difference schema lets us generalize optimization problems on any kind of analytical functions."
      ],
      "metadata": {
        "id": "FVGDCDPOLNfL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hT1IpXQkA9W"
      },
      "outputs": [],
      "source": [
        "def grad_left(F: Callable[[torch.Tensor], torch.Tensor], x: torch.Tensor, h: float = 0.001) -> torch.Tensor:\n",
        "  \"\"\"A finite-difference approximation for left-side gradient ∇F₋(x) with the precision order O(h^2).\n",
        "\n",
        "  Args:\n",
        "      F (Callable[[torch.Tensor], torch.Tensor]): an objective function F(x) with a single input argument x ∈ ℝⁿ.\n",
        "      x (torch.Tensor): an input vector x ∈ ℝⁿ, where the derivative is calculated.\n",
        "      h (float, optional): a step of the derivative partitioning grid with the range of 0<h<1. The lower value, the higher gradient precision. Defaults to 0.001.\n",
        "\n",
        "  Returns:\n",
        "      torch.Tensor: a gradient vector approximation ∇F₋(x).\n",
        "  \"\"\"\n",
        "\n",
        "  pass # TODO: Implement second-order accurate forward difference algorithm\n",
        "\n",
        "\n",
        "def grad_center(F: Callable[[torch.Tensor], torch.Tensor], x: torch.Tensor, h: float = 0.001) -> torch.Tensor:\n",
        "  \"\"\"A finite-difference approximation for central gradient ∇F(x) with the precision order O(h^2).\n",
        "\n",
        "  Args:\n",
        "      F (Callable[[torch.Tensor], torch.Tensor]): a target function F(x) with a single input argument x ∈ ℝⁿ.\n",
        "      x (torch.Tensor): an input vector x ∈ ℝⁿ, where the derivative is calculated.\n",
        "      h (float, optional): a step of the derivative partitioning grid with the range of 0 < h < 1. The lower value, the higher gradient precision. Defaults to 0.001.\n",
        "\n",
        "  Returns:\n",
        "      torch.Tensor: a gradient vector approximation ∇F(x).\n",
        "  \"\"\"\n",
        "\n",
        "  pass # TODO: Implement second-order accurate center difference algorithm\n",
        "\n",
        "\n",
        "def grad_right(F: Callable[[torch.Tensor], torch.Tensor], x: torch.Tensor, h: float = 0.001) -> torch.Tensor:\n",
        "  \"\"\"A finite-difference approximation for right-side gradient ∇F+(x) with the precision order O(h^2).\n",
        "\n",
        "  Args:\n",
        "      F (Callable[[torch.Tensor], torch.Tensor]): a target function F(x) with a single input argument x ∈ ℝⁿ.\n",
        "      x (torch.Tensor): an input vector x ∈ ℝⁿ, where the derivative is calculated.\n",
        "      h (float, optional): a step of the derivative partitioning grid with the range of 0<h<1. The lower value, the higher gradient precision. Defaults to 0.001.\n",
        "\n",
        "  Returns:\n",
        "      torch.Tensor: a gradient vector approximation ∇F+(x).\n",
        "  \"\"\"\n",
        "\n",
        "  pass # TODO: Implement second-order accurate center difference algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb2OyaXMp80i"
      },
      "source": [
        "We will check the accuracy of the implemented method on the test case:\n",
        "\n",
        "$ F(x, y) = x^2 + xy + y^2 \\implies \\begin{cases} \\frac{\\partial F(x, y)}{\\partial x} = 2x + y, \\frac{\\partial F(2.0, -1.0)}{\\partial x} = 3.0 \\\\ \\frac{\\partial F(x, y)}{\\partial y} = x + 2y, \\frac{\\partial F(2.0, -1.0)}{\\partial y} = 0.0\n",
        " \\end{cases} $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8k0BDMmqp7Ub"
      },
      "outputs": [],
      "source": [
        "h = 0.001\n",
        "x = torch.tensor([2.0, -1.0])\n",
        "f = lambda x: x[0] ** 2 + x[0] * x[1] + x[1] ** 2\n",
        "\n",
        "assert torch.allclose(grad_left(f, x, h=h), torch.tensor([3.0, 0.0]), rtol=h)\n",
        "assert torch.allclose(grad_center(f, x, h=h), torch.tensor([3.0, 0.0]), rtol=h)\n",
        "assert torch.allclose(grad_right(f, x, h=h), torch.tensor([3.0, 0.0]), rtol=h)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}