{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b67d3b57-3861-4767-b521-70c08e22acee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from torchmetrics.classification import MulticlassROC\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "907841f0-0fe3-4e3b-8ead-4af74684501b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define data transformations, including normalization\n",
    "transformTrain = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "])\n",
    "\n",
    "transformTest = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transformTrain)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transformTest)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "254f0fa2-9e9e-4e50-8a2c-19752c0c6999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee758f6-c101-45fc-addf-f5aba3057022",
   "metadata": {},
   "source": [
    "### Gradient Sampling\n",
    "\n",
    "One of the newest approaches in general NSO is to use gradient sampling algorithms developed by Burke. The gradient sampling method (GS) is a method for minimizing an objective function that is locally Lipschitz continuous and smooth on an open dense subset $D \\in \\mathbb{R}^n$. The objective may be nonsmooth and/or nonconvex. The GS may be considered as a stabilized steepest descent algorithm. The central idea behind these techniques is to approximate the subdifferential of the objective function through random sampling of gradients near the current iteration point. The ongoing progress in the development of gradient sampling algorithms  suggests that they may have potential to rival bundle methods in the terms of theoretical might and practical performance. \n",
    "\n",
    "\n",
    "Let $f$ be a locally Lipschitz continuous function on $\\mathbb{R}^n$, and suppose that $f$ is smooth on an open dense subset $D \\in \\mathbb{R}^n$. In addition, assume that there exists a point such that the level set $lev_{ƒ(\\bar{x})} = \\{x | f(x)≤ f(\\bar{x})\\}$ is compact. At a given iterate $x_k$ the gradient of the objective function is computed on a set of randomly generated nearby points $u_{kj}$ with $j \\in \\{1, 2,\\ldots, m\\}$ and $m > n + 1$. This information is utilized to construct a search direction as a vector in the convex hull of these gradients with the shortest norm. A standard line search is then used to obtain a point with lower objective function value. The stabilization of the method is controlled by the sampling radius ε used to sample the gradients.\n",
    "The pseudo-code of the GS is the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae026f-56c9-4881-af06-f554c30d60d7",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent\n",
    "\n",
    "Stochastic gradient descent (SGD) calculates gradient and performs parameters update for a single training example at once $( x_{i1}, ..., x_{in}, y_{i} ) $, where $ x_{i1}, ..., x_{in} $ are input features of a training example and $ y_{i} $ - label a training example: $\\theta^{(i+1)} = \\theta^{(i)} - \\lambda \\nabla_{\\theta} J(\\theta_{1}^{(i)}, ..., \\theta_{n}^{(i)}, x_{1}^{(i)}, ..., x_{n}^{(i)}, y^{(i)})$\n",
    "\n",
    "SGD removes redundancy for recomputing gradients for similar examples before each parameter update and also provides a possibility for a parallel computation of the gradient value for each example, which leads to updating model parameters \"on-the-fly\" (which lead to the faster computation).\n",
    "\n",
    "Nevertheless, this approach has its own flaw: frequent updates with a high variance that cause the objective function to fluctuate heavily On the one hand, SGD's fluctuation enables it to jump to a new and potentially better (global) minimum in a process of descending to the other local minimum. But on the other hand, this ultimately complicates convergence to the exact minimum, as SGD will keep overshooting. In practice, this flaw can be solved using learning rate decay methods (e. g. rate decreasing over epoch/iteration), though it requires precise parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65bde898-34c4-43b2-b2ba-93ce18883b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SgdTest(torch.optim.Optimizer):\n",
    "\n",
    "    # Init Method:\n",
    "    def __init__(self, params, lr=1e-2, K=10, h=1e-10):\n",
    "      self.K = K\n",
    "      self.h = h\n",
    "      defaults = dict(lr=lr)\n",
    "      super().__init__(params, defaults)\n",
    "\n",
    "\n",
    "    # Step Method\n",
    "    def step(self, closure=None):\n",
    "      assert closure is not None, 'Closure function is required'\n",
    "\n",
    "      loss1 = closure()\n",
    "\n",
    "      #Update params\n",
    "      for i, group in enumerate(self.param_groups):\n",
    "        lr = group['lr']\n",
    "        for j, p in enumerate(group['params']):\n",
    "          #p.data = p.data - lr * g_group_grads[i][j]\n",
    "          p.data.add_(p.grad.data, alpha = -lr)\n",
    "      return loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "831d1ed6-5b89-4348-bf9e-1c7183781b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SgdGradientSampling(torch.optim.Optimizer):\n",
    "\n",
    "    # Init Method:\n",
    "    def __init__(self, params, lr=1e-2, K=10, h=1e-10):\n",
    "      self.K = K\n",
    "      self.h = h\n",
    "      defaults = dict(lr=lr)\n",
    "      super().__init__(params, defaults)\n",
    "\n",
    "\n",
    "    # Step Method\n",
    "    def step(self, closure=None):\n",
    "      assert closure is not None, 'Closure function is required'\n",
    "\n",
    "      loss1 = closure()\n",
    "\n",
    "      params_clone = list()\n",
    "      k_group_grads = list()\n",
    "      group_grads = list()\n",
    "        \n",
    "      # Generate copies of original param values and initialize update values variable\n",
    "      for group in self.param_groups:\n",
    "        grads = list()\n",
    "        group_clone = list()\n",
    "        for p in group['params']:\n",
    "          group_clone.append(p.data.detach().clone())\n",
    "          grad = p.grad.data.clone()\n",
    "          grads.append(grad)\n",
    "        group_grads.append(grads)\n",
    "        params_clone.append(group_clone)\n",
    "          \n",
    "      k_group_grads.append(group_grads)\n",
    "\n",
    "\n",
    "      # Samples and grad\n",
    "      for _ in range(self.K):\n",
    "        group_grads = list()\n",
    "          \n",
    "        for i, group in enumerate(self.param_groups):\n",
    "          grads = list()\n",
    "          noises = list()\n",
    "          for j, p in enumerate(group['params']):\n",
    "            noise = torch.rand_like(p.data, device=p.device)\n",
    "            noise = torch.norm(noise, p=2)\n",
    "            p.data.add_(noise,alpha=self.h)\n",
    "            #grad = p.grad.data.clone()\n",
    "            #grads.append(grad)\n",
    "            #p.data = params_clone[i][j].clone()\n",
    "          #group_grads.append(grads)\n",
    "\n",
    "        closure()\n",
    "          \n",
    "        for i, group in enumerate(self.param_groups):\n",
    "          grads = list()\n",
    "          noises = list()\n",
    "          for j, p in enumerate(group['params']):\n",
    "            grad = p.grad.data.clone()\n",
    "            grads.append(grad)\n",
    "            p.data = params_clone[i][j].clone()\n",
    "          group_grads.append(grads)\n",
    "        k_group_grads.append(group_grads)\n",
    "\n",
    "\n",
    "\n",
    "      g_group_grads = list()\n",
    "\n",
    "      #Calculate gk\n",
    "      for i, group in enumerate(k_group_grads[0]):\n",
    "        g_grads = list()\n",
    "        for j, grad in enumerate(group):\n",
    "          k = 0\n",
    "          arg_min = list()\n",
    "          while k < len(k_group_grads)-1:\n",
    "            norm = torch.norm(k_group_grads[k][i][j])**2\n",
    "            arg_min.append(norm.item())\n",
    "            k = k + 1\n",
    "          index_min = np.array(arg_min).argmin()\n",
    "          g_grads.append(k_group_grads[index_min][i][j])\n",
    "        g_group_grads.append(g_grads)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      #Update params\n",
    "      for i, group in enumerate(self.param_groups):\n",
    "        lr = group['lr']\n",
    "        for j, p in enumerate(group['params']):\n",
    "          #p.data = p.data - lr * g_group_grads[i][j]\n",
    "          p.data.add_(g_group_grads[i][j], alpha = -lr)\n",
    "      return loss1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645e9a61-45b4-4675-8630-249d9c14d70d",
   "metadata": {},
   "source": [
    "## Accelerated gradient descent methods\n",
    "\n",
    "As was highlighted before, the common problems of the vanilla gradient descent variations are **learning rate tuning** and loss function fluctuations (for SGD and MGD). The recent researches in GD algorithms shown, that these problems can be solved and the convergence rate can be increased by using adaptive gradient calculation, which means applying gradient values from previous iterations to adjust the current one.\n",
    "\n",
    "### Momentum\n",
    "\n",
    "Returning back to the analogy of gradient descent algorithm as a ball, rolling down the hill, important to mention that it's not rolling with a constant speed $ \\lambda $, but instead gaining speed with time by keeping the momentum of itself. The same approach was discovered in optimization theory: the convergence rate of an algorithm can be increased by applying a fraction $ \\gamma $ of gradient from the previous iteration:\n",
    "\n",
    "$ v_{t} = \\gamma v_{t-1} + \\lambda \\nabla_{\\theta} J(\\theta_{1}^{(i)}, ..., \\theta_{n}^{(i)}, x_{1}^{(i)}, ..., x_{n}^{(i)}, y^{(i)}) $\n",
    "\n",
    "$ \\theta^{(i+1)} = \\theta^{(i)} - v_{t} $\n",
    "\n",
    "The momentum term $ \\gamma $ is usually set to 0.9 or a similar value.\n",
    "\n",
    "As mentioned in previous sections, SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around the local minimum. Momentum helps accelerate SGD in the relevant direction and dampens oscillations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbf26de0-869d-4216-8d76-34159c094f93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MomentumGradientSampling(torch.optim.Optimizer):\n",
    "\n",
    "    # Init Method:\n",
    "    def __init__(self, params, lr=1e-2, K=10, h=1e-10, momentum = 0.9, velocity = list()):\n",
    "      self.K = K\n",
    "      self.h = h\n",
    "      self.momentum = momentum\n",
    "      self.velocity = velocity\n",
    "      defaults = dict(lr=lr)\n",
    "      super().__init__(params, defaults)\n",
    "\n",
    "\n",
    "    # Step Method\n",
    "    def step(self, closure=None):\n",
    "      assert closure is not None, 'Closure function is required'\n",
    "\n",
    "      loss1 = closure()\n",
    "\n",
    "      if len(self.velocity) == 0:\n",
    "        for group in self.param_groups:\n",
    "          v_group = list()\n",
    "          for p in group['params']:\n",
    "            v_group.append(torch.zeros_like(p.data))\n",
    "          self.velocity.append(v_group)\n",
    "\n",
    "      params_clone = list()\n",
    "      k_group_grads = list()\n",
    "      group_grads = list()\n",
    "\n",
    "      for group in self.param_groups:\n",
    "        grads = list()\n",
    "        group_clone = list()\n",
    "        for p in group['params']:\n",
    "          group_clone.append(p.data.detach().clone())\n",
    "          grad = p.grad.data.clone()\n",
    "          grads.append(grad)\n",
    "        group_grads.append(grads)\n",
    "        params_clone.append(group_clone)\n",
    "          \n",
    "      k_group_grads.append(group_grads)\n",
    "\n",
    "      # Samples and grad\n",
    "      for _ in range(self.K):\n",
    "        group_grads = list()\n",
    "          \n",
    "        for i, group in enumerate(self.param_groups):\n",
    "          grads = list()\n",
    "          noises = list()\n",
    "          for j, p in enumerate(group['params']):\n",
    "            noise = torch.rand_like(p.data, device=p.device)\n",
    "            noise = torch.norm(noise, p=2)\n",
    "            p.data.add_(noise,alpha=self.h)\n",
    "            #grad = p.grad.data.clone()\n",
    "            #grads.append(grad)\n",
    "            #p.data = params_clone[i][j].clone()\n",
    "          #group_grads.append(grads)\n",
    "\n",
    "        closure()\n",
    "          \n",
    "        for i, group in enumerate(self.param_groups):\n",
    "          grads = list()\n",
    "          noises = list()\n",
    "          for j, p in enumerate(group['params']):\n",
    "            grad = p.grad.data.clone()\n",
    "            grads.append(grad)\n",
    "            p.data = params_clone[i][j].clone()\n",
    "          group_grads.append(grads)\n",
    "        k_group_grads.append(group_grads)\n",
    "\n",
    "\n",
    "\n",
    "      g_group_grads = list()\n",
    "\n",
    "      #Calculate gk\n",
    "      for i, group in enumerate(k_group_grads[0]):\n",
    "        g_grads = list()\n",
    "        for j, grad in enumerate(group):\n",
    "          k = 0\n",
    "          arg_min = list()\n",
    "          while k < len(k_group_grads)-1:\n",
    "            norm = torch.norm(k_group_grads[k][i][j])**2\n",
    "            arg_min.append(norm.item())\n",
    "            k = k + 1\n",
    "          index_min = np.array(arg_min).argmin()\n",
    "          g_grads.append(k_group_grads[index_min][i][j])\n",
    "        g_group_grads.append(g_grads)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      #Update params\n",
    "      for i, group in enumerate(self.param_groups):\n",
    "        lr = group['lr']\n",
    "        for j, p in enumerate(group['params']):\n",
    "          v = self.momentum * self.velocity[i][j] + lr * g_group_grads[i][j]\n",
    "          p.data.sub_(v)\n",
    "          self.velocity[i][j] = v.clone()\n",
    "            \n",
    "      return loss1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3383f4-ddc6-499f-a716-910c79e77212",
   "metadata": {},
   "source": [
    "### Nesterov accelerated gradient\n",
    "\n",
    "In this approach, the momentum is not only applied to the calculated gradient value on the iteration, but also to the current parameters, which gives an approximation of their next position, like a preliminary step of the algorithm:\n",
    "\n",
    "$ v_{i} = \\gamma v_{i-1} + \\lambda \\nabla_{\\theta} J(\\theta^{(i)} - \\gamma v_{i-1}) $\n",
    "\n",
    "$ \\theta^{(i+1)} = \\theta^{(i)} - v_{i} $\n",
    "\n",
    "Taking the analogy from above, if a ball is made of light material, it may slope up to the other side of a hill using its momentum. But if it is made of heavy material (like steel), it will slow down near the bottom of the hill. The NAG algorithm work by the same principle. While momentum first computes the current gradient and then takes a big jump in the direction of the updated accumulated gradient, NAG first makes a big jump in the direction of the previously accumulated gradient, measures the gradient, and then makes a correction. This anticipatory update prevents us from going too fast and results in increased responsiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2237cb34-aaca-420f-97a3-8eb19a924369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NagGradientSampling(torch.optim.Optimizer):\n",
    "\n",
    "    # Init Method:\n",
    "    def __init__(self, params, lr=1e-2, K=10, h=1e-10, momentum = 0.9, velocity = list(), prev_grads = list()):\n",
    "      self.K = K\n",
    "      self.h = h\n",
    "      self.momentum = momentum\n",
    "      self.velocity = velocity\n",
    "      self.prev_grads = prev_grads\n",
    "      defaults = dict(lr=lr)\n",
    "      super().__init__(params, defaults)\n",
    "\n",
    "\n",
    "    # Step Method\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "      assert closure is not None, 'Closure function is required'\n",
    "\n",
    "      loss1 = closure()\n",
    "\n",
    "      if len(self.velocity) == 0:\n",
    "        for group in self.param_groups:\n",
    "          v_group = list()\n",
    "          for p in group['params']:\n",
    "            v_group.append(torch.zeros_like(p.data))\n",
    "          self.velocity.append(v_group)\n",
    "\n",
    "      params_clone = list()\n",
    "      k_group_grads = list()\n",
    "      group_grads = list()\n",
    "\n",
    "      for group in self.param_groups:\n",
    "        grads = list()\n",
    "        group_clone = list()\n",
    "        for p in group['params']:\n",
    "          group_clone.append(p.data.detach().clone())\n",
    "          grad = p.grad.data.clone()\n",
    "          grads.append(grad)\n",
    "        group_grads.append(grads)\n",
    "        params_clone.append(group_clone)\n",
    "\n",
    "      k_group_grads.append(group_grads)\n",
    "\n",
    "      for _ in range(self.K):\n",
    "        group_grads = list()\n",
    "          \n",
    "        for i, group in enumerate(self.param_groups):\n",
    "          grads = list()\n",
    "          noises = list()\n",
    "          for j, p in enumerate(group['params']):\n",
    "            noise = torch.rand_like(p.data, device=p.device)\n",
    "            noise = torch.norm(noise, p=2)\n",
    "            p.data.add_(noise, alpha = self.h)\n",
    "            p.data.add_(self.velocity[i][j], alpha = -self.momentum)\n",
    "            #grad = p.grad.data.clone()\n",
    "            #grads.append(grad)\n",
    "            #p.data = params_clone[i][j].clone()\n",
    "          #group_grads.append(grads)\n",
    "\n",
    "        closure()\n",
    "          \n",
    "        for i, group in enumerate(self.param_groups):\n",
    "          grads = list()\n",
    "          noises = list()\n",
    "          for j, p in enumerate(group['params']):\n",
    "            grad = p.grad.data.clone()\n",
    "            grads.append(grad)\n",
    "            p.data = params_clone[i][j].clone()\n",
    "          group_grads.append(grads)\n",
    "        k_group_grads.append(group_grads)\n",
    "\n",
    "\n",
    "\n",
    "      g_group_grads = list()\n",
    "\n",
    "      #Calculate gk\n",
    "      for i, group in enumerate(k_group_grads[0]):\n",
    "        g_grads = list()\n",
    "        for j, grad in enumerate(group):\n",
    "          k = 0\n",
    "          arg_min = list()\n",
    "          while k < len(k_group_grads)-1:\n",
    "            norm = torch.norm(k_group_grads[k][i][j])**2\n",
    "            arg_min.append(norm.item())\n",
    "            k = k + 1\n",
    "          index_min = np.array(arg_min).argmin()\n",
    "          g_grads.append(k_group_grads[index_min][i][j])\n",
    "        g_group_grads.append(g_grads)\n",
    "\n",
    "\n",
    "      #Update params\n",
    "      for i, group in enumerate(self.param_groups):\n",
    "        lr = group['lr']\n",
    "        for j, p in enumerate(group['params']):\n",
    "\n",
    "          v_half = self.momentum * self.velocity[i][j] + lr * g_group_grads[i][j]\n",
    "\n",
    "          #p.data = p.data - v_half\n",
    "          p.data.sub_(v_half)\n",
    "\n",
    "          self.velocity[i][j] = v_half\n",
    "\n",
    "      return loss1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ebf47a1-cf2d-4df3-adf2-fb799e98e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a CNN model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec17073d-6e28-4645-80e3-e56f0d3c3aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10CnnModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "            nn.BatchNorm2d(128),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
    "            nn.BatchNorm2d(256),\n",
    "\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(256*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e27ea29-006b-4804-9a21-3067d03297cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainModel(model, optimizer, EPOCHS=10):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    gs_losses = list()\n",
    "    min_loss = float(\"inf\")\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            #outputs = model(inputs)\n",
    "            #loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "            #loss.backward()\n",
    "\n",
    "            def closure():\n",
    "                y_hat = model(inputs)\n",
    "                loss = criterion(y_hat, labels)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "\n",
    "            loss = optimizer.step(closure=closure)\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            #_, predicted = outputs.max(1)\n",
    "            #total += labels.size(0)\n",
    "            #correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            if i and not i % 100:    # print every 2000 mini-batches\n",
    "              print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "              gs_losses.append(running_loss / 100)\n",
    "              running_loss = 0.0\n",
    "\n",
    "            #Clear memory\n",
    "            #del inputs, labels, outputs, loss\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "\n",
    "        #with torch.no_grad():\n",
    "            #model.eval()\n",
    "            #val_loss = 0.0\n",
    "            #for val_data in testloader:\n",
    "                #val_inputs, val_labels = val_data[0].to(device), val_data[1].to(device)\n",
    "                #val_outputs = model(val_inputs)\n",
    "                #val_loss += criterion(val_outputs, val_labels).item()\n",
    "                #del val_inputs, val_labels, val_outputs\n",
    "                #torch.cuda.empty_cache()\n",
    "            #val_loss /= len(testloader)\n",
    "\n",
    "            #if val_loss < min_loss:\n",
    "                #min_loss = val_loss\n",
    "            #else:\n",
    "                #break\n",
    "        \n",
    "        #model.train()\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "    # Lists to store true labels and predicted probabilities\n",
    "    true_labels = []\n",
    "    predicted_probs = []\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    predicted_probs = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            predicted_probs.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    predicted_probs = torch.tensor(np.array(predicted_probs))\n",
    "    true_labels = torch.tensor(np.array(true_labels))      \n",
    "\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
    "\n",
    "    gs_score = correct / total\n",
    "    \n",
    "    return gs_score, gs_losses, predicted_probs, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c911b269-d027-4d76-84ee-8d6adb0bb5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModelOriginal(model, optimizer, EPOCHS=10):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    gs_losses = list()\n",
    "    min_loss = float(\"inf\")\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            #_, predicted = outputs.max(1)\n",
    "            #total += labels.size(0)\n",
    "            #correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            if i and not i % 100:    # print every 2000 mini-batches\n",
    "              print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "              gs_losses.append(running_loss / 100)\n",
    "              running_loss = 0.0\n",
    "\n",
    "            #Clear memory\n",
    "            #del inputs, labels, outputs, loss\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "\n",
    "        #with torch.no_grad():\n",
    "            #model.eval()\n",
    "            #val_loss = 0.0\n",
    "            #for val_data in testloader:\n",
    "                #val_inputs, val_labels = val_data[0].to(device), val_data[1].to(device)\n",
    "                #val_outputs = model(val_inputs)\n",
    "                #val_loss += criterion(val_outputs, val_labels).item()\n",
    "                #del val_inputs, val_labels, val_outputs\n",
    "                #torch.cuda.empty_cache()\n",
    "            #val_loss /= len(testloader)\n",
    "\n",
    "            #if val_loss < min_loss:\n",
    "                #min_loss = val_loss\n",
    "            #else:\n",
    "                #break\n",
    "        \n",
    "        #model.train()\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "    # Lists to store true labels and predicted probabilities\n",
    "    true_labels = []\n",
    "    predicted_probs = []\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    predicted_probs = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            predicted_probs.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    predicted_probs = torch.tensor(np.array(predicted_probs))\n",
    "    true_labels = torch.tensor(np.array(true_labels))      \n",
    "\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
    "\n",
    "    gs_score = correct / total\n",
    "    \n",
    "    return gs_score, gs_losses, predicted_probs, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7ad148-363c-4793-8666-6f9ddab2e97e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   101] loss: 1.948\n",
      "[1,   201] loss: 1.658\n",
      "[1,   301] loss: 1.555\n",
      "[1,   401] loss: 1.464\n",
      "[1,   501] loss: 1.382\n",
      "[1,   601] loss: 1.327\n",
      "[1,   701] loss: 1.282\n",
      "[2,   101] loss: 1.221\n",
      "[2,   201] loss: 1.142\n",
      "[2,   301] loss: 1.121\n",
      "[2,   401] loss: 1.093\n",
      "[2,   501] loss: 1.070\n",
      "[2,   601] loss: 1.039\n",
      "[2,   701] loss: 1.014\n",
      "[3,   101] loss: 0.996\n",
      "[3,   201] loss: 0.928\n",
      "[3,   301] loss: 0.914\n",
      "[3,   401] loss: 0.928\n",
      "[3,   501] loss: 0.886\n",
      "[3,   601] loss: 0.876\n",
      "[3,   701] loss: 0.888\n",
      "[4,   101] loss: 0.846\n",
      "[4,   201] loss: 0.835\n",
      "[4,   301] loss: 0.820\n",
      "[4,   401] loss: 0.816\n",
      "[4,   501] loss: 0.798\n",
      "[4,   601] loss: 0.804\n",
      "[4,   701] loss: 0.810\n",
      "[5,   101] loss: 0.784\n",
      "[5,   201] loss: 0.757\n",
      "[5,   301] loss: 0.761\n",
      "[5,   401] loss: 0.745\n",
      "[5,   501] loss: 0.733\n",
      "[5,   601] loss: 0.745\n",
      "[5,   701] loss: 0.740\n",
      "[6,   101] loss: 0.717\n",
      "[6,   201] loss: 0.719\n",
      "[6,   301] loss: 0.686\n",
      "[6,   401] loss: 0.695\n",
      "[6,   501] loss: 0.664\n",
      "[6,   601] loss: 0.698\n",
      "[6,   701] loss: 0.690\n",
      "[7,   101] loss: 0.646\n",
      "[7,   201] loss: 0.665\n",
      "[7,   301] loss: 0.677\n",
      "[7,   401] loss: 0.655\n",
      "[7,   501] loss: 0.656\n",
      "[7,   601] loss: 0.654\n",
      "[7,   701] loss: 0.633\n",
      "[8,   101] loss: 0.621\n",
      "[8,   201] loss: 0.643\n",
      "[8,   301] loss: 0.612\n",
      "[8,   401] loss: 0.616\n",
      "[8,   501] loss: 0.606\n",
      "[8,   601] loss: 0.624\n",
      "[8,   701] loss: 0.610\n",
      "[9,   101] loss: 0.581\n",
      "[9,   201] loss: 0.590\n",
      "[9,   301] loss: 0.572\n",
      "[9,   401] loss: 0.574\n",
      "[9,   501] loss: 0.597\n",
      "[9,   601] loss: 0.571\n",
      "[9,   701] loss: 0.595\n",
      "[10,   101] loss: 0.570\n",
      "[10,   201] loss: 0.548\n",
      "[10,   301] loss: 0.553\n",
      "[10,   401] loss: 0.551\n",
      "[10,   501] loss: 0.561\n",
      "[10,   601] loss: 0.569\n",
      "[10,   701] loss: 0.552\n",
      "[11,   101] loss: 0.548\n"
     ]
    }
   ],
   "source": [
    "#NagGradientSampling\n",
    "model = Cifar10CnnModel()\n",
    "model.to(device)\n",
    "optimizer = NagGradientSampling(model.parameters(), lr=1e-2, K=50, h=1e-10, momentum = 0.9)\n",
    "gs_nag_score, gs_nag_losses, gs_nag_pred, gs_nag_true = trainModel(model, optimizer, 30)\n",
    "\n",
    "#MomentumGradientSampling\n",
    "#model = Cifar10CnnModel()\n",
    "#model.to(device)\n",
    "#optimizer = MomentumGradientSampling(model.parameters(), lr=1e-2, K=30, h=1e-6, momentum = 0.9)\n",
    "#gs_momentum_score, gs_momentum_losses, gs_momentum_pred, gs_momentum_true = trainModel(model, optimizer, 30)\n",
    "\n",
    "#SgdGradientSampling\n",
    "#model = Cifar10CnnModel()\n",
    "#model.to(device)\n",
    "#optimizer = SgdGradientSampling(model.parameters(), lr=1e-2, K=100, h=1e-3)\n",
    "#gs_sgd_score, gs_sgd_losses, gs_sgd_pred, gs_sgd_true = trainModel(model, optimizer, 30)\n",
    "\n",
    "#Nag\n",
    "model = Cifar10CnnModel()\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, nesterov=True)\n",
    "nag_score, nag_losses, nag_pred, nag_true = trainModelOriginal(model, optimizer, 30)\n",
    "\n",
    "#Momentum\n",
    "#model = Cifar10CnnModel()\n",
    "#model.to(device)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "#momentum_score, momentum_losses, momentum_pred, momentum_true = trainModelOriginal(model, optimizer, 30)\n",
    "\n",
    "#Sgd\n",
    "#model = Cifar10CnnModel()\n",
    "#model.to(device)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "#sgd_score, sgd_losses, sgd_pred, sgd_true = trainModelOriginal(model, optimizer,30)\n",
    "\n",
    "#SgdTest\n",
    "#model = Cifar10CnnModel()\n",
    "#model.to(device)\n",
    "#optimizer = SgdTest(model.parameters(), lr=0.01)\n",
    "#sgd_score, sgd_losses, sgd_pred, sgd_true = trainModel(model, optimizer,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e689b9f-c7ab-4817-90fd-df9960bbd4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nag Gradient Sampling\")\n",
    "metric = MulticlassROC(num_classes=10, thresholds=None)\n",
    "fpr, tpr, thresholds = metric(gs_nag_pred, gs_nag_true)\n",
    "fig_, ax_ = metric.plot(score=True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Momentum Gradient Sampling\")\n",
    "metric = MulticlassROC(num_classes=10, thresholds=None)\n",
    "fpr, tpr, thresholds = metric(gs_momentum_pred, gs_momentum_true)\n",
    "fig_, ax_ = metric.plot(score=True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Sgd Gradient Sampling\")\n",
    "metric = MulticlassROC(num_classes=10, thresholds=None)\n",
    "fpr, tpr, thresholds = metric(gs_sgd_pred, gs_sgd_true)\n",
    "fig_, ax_ = metric.plot(score=True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Nag\")\n",
    "metric = MulticlassROC(num_classes=10, thresholds=None)\n",
    "fpr, tpr, thresholds = metric(nag_pred, nag_true)\n",
    "fig_, ax_ = metric.plot(score=True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Momentum\")\n",
    "metric = MulticlassROC(num_classes=10, thresholds=None)\n",
    "fpr, tpr, thresholds = metric(momentum_pred, momentum_true)\n",
    "fig_, ax_ = metric.plot(score=True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Sgd\")\n",
    "metric = MulticlassROC(num_classes=10, thresholds=None)\n",
    "fpr, tpr, thresholds = metric(sgd_pred, sgd_true)\n",
    "fig_, ax_ = metric.plot(score=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacb61f5-60df-4b73-a7d2-76dfcf2c3b4d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(gs_nag_losses, label='Nag Gradient Sampling')\n",
    "plt.plot(gs_momentum_losses, label='Momentum Gradient Sampling')\n",
    "plt.plot(gs_sgd_losses, label='Sgd Gradient Sampling')\n",
    "plt.plot(nag_losses, label='Nag')\n",
    "plt.plot(momentum_losses, label='Momentum')\n",
    "plt.plot(sgd_losses, label='Sgd')\n",
    "\n",
    "plt.title('Gradient descent algorithms comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c0fda6-9a18-401a-b315-3cb450608a92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of accuracy scores\n",
    "accuracy_scores = [gs_nag_score, gs_momentum_score, gs_sgd_score, nag_score, momentum_score, sgd_score]\n",
    "\n",
    "# Corresponding labels or methods\n",
    "methods = ['Nag Gradient Sampling', 'Momentum Gradient Sampling', 'Sgd Gradient Sampling', 'Nag', 'Momentum', 'Sgd']\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(methods, accuracy_scores, color='skyblue')\n",
    "plt.xlabel('Accuracy Score')\n",
    "plt.title('Accuracy Scores for Different Methods')\n",
    "plt.xlim(0, 1)  # Set the x-axis limits from 0 to 1 for accuracy scores\n",
    "plt.gca().invert_yaxis()  # Invert the y-axis to display the highest accuracy at the top\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "\n",
    "# Display the values on the bars\n",
    "for i, score in enumerate(accuracy_scores):\n",
    "    plt.text(score + 0.01, i, f'{score:.4f}', va='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c81c158-470b-4016-8009-b8b7d9dbc998",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Example X and Y arrays\n",
    "X = [1, 2, 3, 4, 5]\n",
    "Y = [10, 20, 30, 40, 50]\n",
    "\n",
    "# Writing X and Y arrays to a CSV file\n",
    "def create_csv_two(X, Y, name):\n",
    "    with open(name + '.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['X', 'Y'])  # Writing header\n",
    "        for i in range(len(X)):\n",
    "            writer.writerow([X[i], Y[i]])\n",
    "\n",
    "def create_csv_one(data, name):\n",
    "    with open(name + '.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Data'])  # Writing header\n",
    "        for value in data:\n",
    "            writer.writerow([value])\n",
    "\n",
    "create_csv_one(gs_nag_losses,'gs_nag_losses')\n",
    "create_csv_one(gs_momentum_losses,'gs_momentum_losses')\n",
    "create_csv_one(gs_sgd_losses,'gs_sgd_losses')\n",
    "\n",
    "create_csv_one(nag_losses,'nag_losses')\n",
    "create_csv_one(momentum_losses,'momentum_losses')\n",
    "create_csv_one(sgd_losses,'sgd_losses')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
